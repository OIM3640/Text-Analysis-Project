# Text-Analysis-Project
 
Please read the [instructions](instructions.md).

**1. Project Overview**

In this project, I have tried multiple sources, including project Gutenberg, Wikipedia, and Newspaper articles. The results of the extracted texts are shown in part one. Out of the multiple resources, I decided to used Wikipedia as my primary text source, and in particular, I want to focus on the Wikipedia for the Babson College. There are many various colleges in the United States, and they have various different rankings. I want to see the different in terms of how the articles are written in the Wikipedia page, and how might it different from lets say, a more prestigious school such as Harvard University. I will be using various techiques and process to analyze the text. The techiques and processes are mainly separated into three categories. The first being the text processes we have covered in class, the second being the methods given in the instructions page, and the third being methods I learned from a different machine learning class along with help from chatGDP. The end goal of this project is to learn whether or not there is a signifcant biasness when it comes to writing an article between a pretigious Ivy League university and a normal ranking college. 

**2. Implementation** 

When extracting data, especially text data, the first thing I decided to do is clean the text source. I did the cleaning by using two different functions. One is used to remove all the punctuation in the text, and the other is to remove all the stop words in the text through the use of gensim python package. After the text is cleaned, I take the cleaned text and made it into a dictionary, with unique word as the key and frequency as corresponding values. Then used both different cases of analysis. As mentioned previously, the first few process and implementations are covered in the class. I used that to determine the most frequent words in the cleaned text, and sort them in a ranking of 10, from most to least. The second part of the implementations are the ones given in the instructions. Some of which I used are nautral language processing to examine whether the article has a negative connotation, and text similarity, which allows me to examine whether or not the two Articles are similar by evaluating different ratios. Lastly, the third part of the implementaion are more of a visual approach. I experimented with some other packages such as networkx to observe the connection between words, and wordcloudx to observe the frequency of the words using a word cloud. By going through these processes and understanding them, it helps us generate a bigger picture. 

There are many points in the project that I have to make a design deicion. One instance in particular is when I try to decide which word network would be the best for this specific kind of mondel, such that it would be more closely related with our course. When asking chatGDP for different kinds word network model and different packages were suggested. The two that are more interesting NetworkX and spaCy. While doing some research of the packages, networkx is more well known and it is easier to use given the format of the data. It provides a built in graphical methods algorithms and data structures for bigrams and trigrams, which we have briefly covered in class. This becomes a useful tool when we are trying to analyze the relationship between given words and what kinds of words are more linkly to be linked together. After making that decision, I need to incorporate such method given my current text dataset. The session of changing the code and adapting the code using chatGDP is provided as screenshot .png files in the images folder. The resulting graph is a bit difficult to observe, and it could be cleaned up a bit more. However, with my current skillset and limited time, it is hard to implement a more advanced word network. 

**3. Results**

I start with a simple frequency analysis on cleaned text and discovered the most frequent word used in the Babson wikipedia page is Babson, with a frequency of 43 and the most frequent word used in the Harvard wikipedia page is Harvard, with a frequency of 115 times. It is expected that both of the College names will appear the most in the text; however, the significant amount of increase in frequency could relate to a higher emphasize on the school title and the school name. Another reason could be the length of the article. Harvard university has a much richer and longer history than Babson College, but this also indiciates the difference in length of a prestigious school compared to a standard college. Other than the college names being the most frequent, when examining the top 10 most appearing words in the created dictionary, I also notices that for Harvard University, the word research appears more than others. While for Babson College,the word mba appears more than others. From this we can see that Harvard University focuses primarily on research and Babson College focuses primarily on mba. Lastly, we can see the world also appears a lot for Harvard University, and this reflects heavily on their global presence. With these information, we see what each school focuses on, and what makes these school stand out more than the others. 

Continuing on with the process and given by the implementations in the instruction page, we are about to use natural language processor to create a sentimental analysis on both of the texts. For the Babson text, the text is approximately .2% negative, 89.9% neutral, and 9.9% positive. In constrast, the analysis indicates that the Harvard text is apprxoimately 4.4% negative, 83.8% netural, and 11.8% positive. Although in both cases the text is being classified majorly as neutral, we can see that the language used in the Harvard text is much diversed in terms of 'feelings' compared the Babson text. The Harvard text has both a more positive and negative connotaion compared to Babson College, and this could indicate that there are some parts of Harvard that is more likable than others. Furthermore, we also compare the similarity between the two texts. The standard fuzz ratio function computes the Levenshtein distance similarity ratio between two text, and in this case, the output is 39%. This ratio can indicate that even though the texts are 39% similar in terms of the words and the order of the words. The partial ratio fuzz performs substring matching. Since Babson college is the shorter string of words, the ratio of 42% indicates that 42% of the text in Babson College is considered substring of Harvard text. Lastly, we examine the token sort ratio using Fuzzy. Although when the text is being tokenized, it cleans the text and then sorted alphabetically joined together, it helps us compare the similarity between two texts without considering order of words. The ratio of 53% indicates that the Babson Text is 53% similar compared to the Harvard text. Out of the three ratios and given word orders matter, the ratio of 39% holds more value than the other two. From this we can see that even thought but texts are about colleges, the focus and the content between two texts are very different. 

Lastly, we will examine the graph and visual outputs given the two text source. All of the data and result are in the images folder for viewing. The first visualization is a word network for the Babson text. There are different types of graph within the library, but the one I chose for the project is Digraph, which is the standard graph with directional vertices. Each node in network graph is a unique word and the size of the circle is based on the weight of the unique word. The bigger the circle indicate a heavier weight of that particular word, and the directional vertices between the words indicate their relationship. For instance, in our case, althete is pointed towards the word website. This means that althete is highly likely to be associated to website, such that 'althete website', the bigram is highly likely to occur in the text. However, there are too many unique words in our text, so we will mainly be looking at clusters of highly weighted words for both texts. The cluster of words represent that those words are closely related to each other. In the Babson text, some words that are closely related to 'Babson' are students, founder, and better. Some words that are closely related to MBA are section and honorary. Some words that are closely related to college are automatic, corporation, and army. On the other hand, in the Harvard text, some words that are closely related to 'Harvard' are wyss, additional, and private. Some words that are closely related to word research are schools and civil. Some words that are related to word university are congregations and prodominately. Comparing different clusters of words we can see how words are related to each other in the text. To generalize for Babson, the content in the Babson Wikipedia page contains relating to higher education and business environment, whil the content in the Harvard wikipedia page is closely related to be a private and research institution. The word clouds are just simply a visualization of the word frequency in terms of sizes among the unique words.

**4. Reflection** 

From a process point view, I believe I have only scratched the surface of the analysis and there are much more to be done and implemented. There are many ways I can improve the code. For instance, I noticed while cleaning the data and and condense them into a string of text, there are some words such as 's' or other stop words such as the school title can be removed as stop words. This will be difficult to generalize when it comes to other school because each school would have a differemt name, and I included this in my analysis because I believe it is necessary to observe and determine the words that are closely associated with the name. I think my project is approxiately scoped and it has a good testing plan, but it is very minimal. There are still various different parameters I can change within the function and methods in order for it to return a reliable result. There are also python libaries that are capable of doing the same job, but due to time limit, I have been unable to play with those and compared the results. I think the testing plan could still be better and improved upon with different other implementations. 

From a learning perspective, I learned about about the text analysis packages and methods that python has to offer. I also become more aware of different structures and debugging while working on the project. It was painful to organize the types of data for different outputs with different functions because some python method implementaions are very specific on the input data. Overall, I believe it teaches me a vaulable lesson of keeping track of data structure and data type. Throughout the project chatGDP has helped me a lot. Many times when I try to implement a method or write a function, sometime will ultimately go wrong and often times, it will output an error message. ChatGDP helps me debug the code and explains to me what the error message means and how I can potentially get rid of the error. Furthermore, ChatGDP not only helped me with code, but it also helped me with other resources. while researching about text analysis, there are many key terms and words that I didnt't understand. ChatGDP help me understand those and explain those concept as well as certain lines of code. With these new knowledges in mind, I can have a better understanding of what text analysis is and use this potential knowledge for my final project. In terms of things I wished I know beforehand are the different grahping libraries. Graphing the data was so goddam painful because of the parameter requirements. I was only about to maybe a very simple basic graph with minimal interpretation. If I had new more, I can perhaps better graph my result. Ultimately, I have learned a lot from this project and heopfully will be using these knowledges in the future. 


*Big big credit to ChatGDP* 