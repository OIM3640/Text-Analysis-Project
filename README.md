# Text-Analysis-Project
### Using computational methods to analyze text.

## Project Overview

 The data source that I used for this porject was from Project Gutenberg, or [gutenberg.org](https://gutenberg.org/). The data included 5 novels by Fyodor Dostoevsky. The novels are the following: "_Notes from the Underground_", "_Crime and Punishment_", _The Brothers Karamazov_", "_The Idiot_", and "_The Possessed_". The data was downloaded from gutenberg.org as .txt files, and saved offline in 5 different .txt files within the project folder "data". The texts range in size, from 4000 to over 28000 lines of text. The goal of my project was to analyze each text, and perform a sentiment analysis to bring more transparency into the language and sentiment of each book. Because the 5 texts were from the same author, I was curious if I could figure out which book was the most positive and which was the most negative. Nineteenth century Russian literature is consistently regarded as bleak, and negative, so, before endugling in these styles of books, I figured I could choose which book to read first based on how postive or negative it was. In order to carry out this sentiment analysis, I broke the project down into 3 stages: **Stage zero** - _gather data_, **Stage one** - _process/clean data_, **Stage two** - _analyze data_.

## Implementation

 **Stage zero:** To begin, I first had to gather the data. Though gutenberg.org and the urllib.request libary make it rather easy to call these online texts into python programs, it became obnoxiously difficult for me to figure out how to parse the text to only include the author's work, and not licenses, titles, etc. This brought about the first design choice I had to make: write a script to do the former, or edit to text files manually. I chose the latter for simplicity's sake, and though it was only for 5 files, I can imagine how tedious it would be to do this for EVERY text you want to examine with my project, aside from these 5 I chose. So, you will notice that the .txt files only include the literature that is worth analyzing, and it could potentially free up some computing power by not having to scrape each text of all licenses or gutenberg.org titles. Once I did this, each text would still have to be cleaned further.

 **Stage one:** After gathering data, I then had to take this raw form of text, stop words, punctuation, whitespace, etc. and only extract the WORDS that were meaningful for my analysis. Additionally, it was imperative that all words would be grouped together, and not seperated by paragraphs, indents, etc. The string library was very efficient in doing this. Utilizing methods like replace(), and lower(), I was able to take each word, remove all line breaks and replace them with a single space, remove all forms of punctuation, make all text lowercase and join and split all the text by one space each. I then had cleaned forms of text data that would be further cleaned with the help of the nltk library. I wrote a function to take each cleaned text file and remove stop words like "and" or "or" and other words that wouldn't aid my sentiment analysis. I stored the data in a list of tuples with each final, fully cleaned, simple form of text, and their respective titles within my program. 

## Results

 **Stage two:** Now that the data was cleaned, I utilized the nltk SentimentIntensityAnalyzer to comb through each text file. This library would analyze a piece of text and the language within it, and then generate a sentiment score for the text in neg (negative), neu (neutral), and pos (positive) values from -1 to 1 for each category. The closer to negative 1, the more negative, and the closer to 1, the more positive. I wrote an iteration function that would pass through the tuple of my stored text and title, and then print the scores for each text when ran. Using f strings, I was able to print the scores and the titles in a simple, aesthetically pleasing manner. 

 After gathering the scores, it was clear that there were some books that were more negative than others, and on the contrary, some that were more positive. The sentiment analyzer proved that the most negative book was "The Idiot", with a neg score of 0.151, and the most positive was "Notes from the Underground" with a pos score of 0.213. After discovering this, I was able to suggest to the reader which book they should read if they were looking for a more dark, or a more upbeat novel. I included this in print statements in my program. 

 I enocuntered an issue after realizing the compound scores of each book would equal 1, and not a range of -1 to 1. This threw me off initially as I believed I was implementing the library correctly, so I reached out to Professor Li and he explained that the library was actually behaving correctly. The area where I could have improved this project was implementing the library in a more impactful way. The nltk library is meant for multiple, short pieces of text, and not typically 20000 lines of text. This makes it increasingly difficult for the library to analyze the sentiment as it usually uses multiple pieces of text to form one sentiment score, not just one piece to form one score. In the future, I would choose to break each book into sections using a dictionary, storing lines as values, and then iterating through each value to form a sentiment score for each book. By breaking the books into a dictionary of smaller segments of text, it may make it easier to figure out which sections of the book are the most positive, most negative, and we can have smaller segments of data to further analyze. 

## Reflection + Use of ChatGPT

 From a process perspective, I think at first I was unclear in which direction to take with this project. Initially, I wanted to do some form of search engine for cybersecurity vulnerabilities, but due to the nature of the database of these vulnerabilities, there was not a lot of text that could be analyzed. I pivoted to this literature lens for analysis after realizing the plethora of text in books can make for some really cool analysis. I mentioned in the prior paragraph a major area that I could have improved with my project, but some more smaller tweaks may make the program perform a lot better. I think I could combine the nltk stopwords cleaning segment of the project within my intial cleaning function to decrease the total lines of code and storage of the text pre-totally cleaned. Additionally, I would utilize a better UI for printing out the scores as the current method with nltk is using a dictionary, and it's not very descriptive.

 ChatGPT was my right hand man in this project for a few purposes. For one, I used ChapGPT to help with the string library and utilizing its methods. I was having trouble removing apostrophes, quotes, single quotes, etc. from the text, and I thought these characters would be under the string.punctuation method. ChatGPT helped me add on an extra piece of code to account for these characters in this cleaning section (see line 50 in analyze_books.py). Additionally, ChatGPT helped me with nltk's stopwords library. I was running into an issue of the spacing between words after removing stopwords from the texts, so ChatGPT helped me remove the words, join all, and then split with a space after carrying out the removal of stopwords (see lines 83-85 in analyze_books.py). All in all, it was helpful having ChatGPT as a resource to help point me in the right direction when I encountered an issue. I did not rely on the tool for writing code, but having it as a supplement to my writing was very helpful. It also helped me figure out how to write this markdown!

 If you're ever looking to read a book, but are too afraid it may be very depressing, or too happy for you, utilize this code to gauge the sentiment of the book before diving in!