# Text-Analysis-Project

Please read the [instructions](instructions.md).

# Project Overview

For this project, I used the MediaWiki API to retrieve and analyze Wikipedia articles on "Real Madrid CF" and "FC Barcelona." The goal was to explore the content of these articles to identify frequently used words and track how often key players—specifically "Ronaldo" for Real Madrid and "Messi" for Barcelona—were mentioned. By examining these word patterns, I hoped to gain insights into each club’s representation on Wikipedia and see if particular words or player mentions could reveal any underlying themes or focus within the articles. The choice to work with Wikipedia as a data source made sense because it provides well-structured, accessible information that’s broadly understood, which allowed me to focus on the text analysis techniques without needing to clean up messy data. Overall, I aimed to strengthen my skills in text processing and understand the common language associated with each team on a public platform like Wikipedia.

# Implementation

The project architecture is split into three main files: "wikiapi.py", "analysis.py", and "main.py", each handling a distinct part of the process. In "wikiapi.py", I used the MediaWiki library to access Wikipedia pages, specifically retrieving the content of the articles based on the page titles. This setup allowed the project to be flexible in pulling other articles if needed, as the structure and retrieval process are universal across Wikipedia pages. Next, "analysis.py" handled the core text processing, where the retrieved text was cleaned by removing punctuation and splitting words. The file includes functions to create a frequency dictionary, determine the most common words, and calculate unique and total word counts for each article. This modular approach made it easy to analyze different articles without rewriting the core analysis functions.

A significant design choice came with how to handle punctuation. Rather than using ASCII-based punctuation removal, I opted for a Unicode-based approach. This choice was made after some research into best practices for text processing in multilingual contexts, as Unicode ensures better compatibility with a wider range of symbols and characters, which are often present in Wikipedia articles that might include international terms or symbols. I realized this would make the word splitting more accurate, especially for pages with special symbols or non-English words, enhancing the quality of the word frequency analysis. I also used GenAI tools to search for examples and strategies for working with text data and Unicode, which helped me troubleshoot and find reliable solutions for handling different types of punctuation efficiently.

# Results

Running the program allowed me to uncover both the total and unique word counts for each page, along with the frequency of mentions for "Ronaldo" and "Messi" in their respective articles. I was able to see that "Ronaldo" was mentioned a certain number of times on the "Real Madrid CF" page, while "Messi" had a different frequency on the "FC Barcelona" page, reflecting the focus on each player within their clubs. The analysis revealed some expected words like "football," "club," and "game," which were commonly found across both articles. However, a few unexpected terms also appeared in the top 10 lists, offering a glimpse into each team’s unique identity as portrayed on Wikipedia. For example, certain words that might reflect historical context or regional importance appeared more prominently for each club, which was interesting to observe.

Having the frequency list of common words helped capture a broad picture of the themes that Wikipedia highlights for each team. If I had more time, I would have visualized these word frequencies through a word cloud or bar graph to better illustrate the differences between the two articles. Presenting this visually would make the patterns more accessible and emphasize the contrasts between the clubs. Despite the lack of visuals, the word frequency data still provided meaningful insights into how both clubs are presented on Wikipedia, particularly in relation to their star players.

# Reflection

Looking back, I’m pleased with how the project turned out, especially with how smoothly the MediaWiki API worked in retrieving the article content. It was easy to access well-organized text data, which made the analysis process more straightforward. One of the main challenges, though, was dealing with Unicode punctuation, which took some trial and error to get right. Initially, I tried a simple ASCII-based approach, but I found that certain characters were missed, which interfered with the accuracy of word counts. Switching to a Unicode-based method ensured that special symbols and dashes were correctly handled, resulting in cleaner data and more reliable results.

From a learning perspective, this project was a great introduction to text processing and data cleaning in Python. Working through each step allowed me to understand the importance of setting up a structured, modular codebase that could be reused for other types of text analysis in the future. Using GenAI tools was particularly helpful when I needed examples of how to structure text projects or troubleshoot specific issues like handling Unicode. For future projects, I’d like to expand on this knowledge by diving deeper into text analysis techniques, such as sentiment analysis or named entity recognition, and plan a more detailed workflow for handling diverse text sources right from the start. Overall, this project gave me a solid foundation to build on, and I feel much more confident approaching similar tasks going forward.
