# Text-Analysis-Project
 
Please read the [instructions](instructions.md).

1. Project Overview 
I used 3 books titled Our War with Spain for Cuba's Freedom, The History of Cuba vol. 2, and Thrilling Narratives of Mutiny, Murder and Piracy from Project Gutenberg. I chose these because I am going on a trip to Cuba over the winter break and I was curious to see what these books said. I also wanted to compare different texts about Cuba because of its turbulent history. I thought this would lead to differing opinions in the texts. I also utilized the IMDB Movie Reviews database when building my sentiment analysis. I found this dataset easier to work with, as the smaller string sizes allowed the run much faster. I used several techniques to analyze the data. Some were more simple, such as comparing the most common words. Others were more difficult and required online research and ChatGPT. However, these were the most rewarding. I especially enjoyed using the Markov text analysis for text generation. A lot of my code for that was from ChatGPT, but after playing around with it I understand it quite a bit now. I hoped to learn good ways to find similarities between seemingly distinct texts, and meaningful ways to measure them.

2. Implementation

What I decided to do was to focus on ways to take massive strings and condense them into something comparable. I wanted to be able to understand the books’ content without actually reading them. Most common words without stop words offer an insight to what the book is saying most of the time, the sentiment analysis was good for getting the overall tone of the book’s subject, and comparing the word count for specific words was useful to see how often topics that interested me was mentioned in the books. These three items were the most important part of my structure. Another major component of my work was the removal of excess information, such as the preamble and stop words. This allowed my research to be much faster and more concise.

I used ChatGPT for code blocks that I had no experience working with, and it ended up influencing my data structure a lot. Initially, I was stumped on how to remove the preamble (I have left a .py file titled failed_key to show how overly complicated I was making it). After using StackOverflow and a bit of ChatGPT to help with compare_word_count, I realized that I could use what I’d just learned for the preamble (make a list of only the words you want and then join it back together into a string). Not only did I gain inspiration for other code after using ChatGPT, but I based the rest of my system on this idea. By having different files that would call the books from part1 I could view one aspect at a time and internalize it. I was originally trying to combine it all into one file, but after seeing the readability of ChatGPT code output because of the spacing I decided to adopt it. This was my first time really using ChatGPT for coding and I had no idea it was this powerful. Honestly, the only reason I was able to utilize the more advanced tools, like text generation using Markov’s chain and the fuzz library, is because of ChatGPT. ChatGPT gives me a tutorial of what the fuzz library is and what its capable of I wanted to utilize the Markov text generation to get reasonably close excerpts from the books. I asked ChatGPT questions on how to generate the closest text groups. Here is the link to my questions: https://chat.openai.com/share/cc46d90a-37af-44e4-b73c-4cbc263a3ad2 


3. Results
I learned several cool things in this project. The first was through the comparative word count. This was beneficial because I was able to understand the tone of the book without reading them. For example, the book about pirate narratives talks a lot about death and the ocean, but almost nothing about treasure, government, or Cuba. This leads me to believe that it is a non-fiction recounting of Caribbean pirates who sometimes found themselves in Cuba. This surprised me, as I searched “Cuba” on Gutenberg library with the intent of getting Cuba-centered books. The word count proved me wrong as the book about Cuba’s fight for freedom mentions Cuba 481 times and the history of Cuba mentions Cuba 628 times, compared to the pirate book’s tiny 6 times. Another thing that shocked me was the amount of times Spain was mentioned, showing me the influence they have had in Cuba. The output is titled "SentimentOutput" in write_up_images.

Another great thing was the sentiment analysis. I was able to predict how the book would reflect its topic based on the score. If the overall tone was more negative, then the subject matter would be tragic. If it was more positive, then it would be proud, or at least reflect a more pleasant time. The most positive text was the book about Cuban history with a positive score of 0.124, showing that the writer sees Cuban history as something Cubans should be proud of. The most negative text was the collection of pirates narratives with a negative score of 0.11. This story is evidently filled with violent, negative stories of said pirates confirming that this is not a romanticized fiction, but a gritty true recollection. The most neutral was the book about Cuba’s war for independence with a neutral score of 0.793, leading me to believe it’s a fact-based book without much emotional investment for either side.  

The last thing I accomplished was building a text generation model using Markov’s text analysis. I relied heavily on ChatGPT for this method, both to see example code and to understand what the various components meant. The larger the n-size the more cohesive the entire story was. What I realized was even stories with an n-size of 1 had a cohesive narrative and gave a good summary of at least a piece of the book. This was also the most enjoyable method as I’ve never made a text generation model. In the write_up_images folder, I have added some of the stories I liked best. They are titled Markov1 and Markov2.
 

 
4. Reflection

I think that overall my process went well. I completed my goal of being able to efficiently digest massive texts into understandable pieces very quickly. The code that I created for this project can be used for any book on Gutenberg and the ability to compare the books within the methods themselves means that digesting multiple books in relation to each other will also be easy. I believe my testing plan was good because I had the necessary results to either confirm my assumptions or confidently make new ones. I also wish I had better time management skills, as I believed my prior coding experience would help me complete this project quickly. It did not.

In terms of ChatGPT, this project really opened my eyes. I have never used ChatGPT for coding before because I am more comfortable with stack overflow and I always believed ChatGPT was more of a crutch than a learning tool. I have completely changed my mind. Not only can you find solutions to complex problems extremely quickly, but it can teach you if given the correct prompts. My understanding of Markov’s text analysis was largely from ChatGPT, and I am now comfortable with a topic I did not even know existed before this project.




