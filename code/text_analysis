import unicodedata


def read_data(filename): 
    """Read all lines from a text file"""
    with open(filename, "r", encoding="utf-8") as file: #r is to just read file，
        data = file.readlines()
        print("Number of lines:", len(data))
        print("Type:", type(data))
        print("First 5 lines preview:\n", data[:5])
    return data

def clean_text(lines): 
    """go through line to line, and drop control control, keep only letters and space"""
    clean = []
    for line in lines: 
        new_line = ""
        for character in line: 
            if unicodedata.category(character).startswith('C'):
                # if character is c, meaning its controled character 
                continue
            if character.isalpha() or character.isspace(): 
                new_line += character.lower()
                # if character is character, add it lowercase to new line"
            else: 
                new_line += ' '
                # if character is puncation, replace with space
        new_line = new_line.strip()   
        if new_line != "":            #GENERATE BY GHATGPT,  Keep non-empty lines only 
            clean.append(new_line)
    
    text = ""
    # join the cleaned lines into one long string separated by spaces
    for line in clean:
        text += line + " "
    return text

stopword = {
    "a","about","above","after","again","against","all","am","an","and","any",
    "are","aren't","as","at","be","because","been","before","being","below",
    "between","both","but","by","can't","cannot","could","couldn't","did",
    "didn't","do","does","doesn't","doing","don't","down","during","each",
    "few","for","from","further","had","hadn't","has","hasn't","have","haven't",
    "having","he","he'd","he'll","he's","her","here","here's","hers","herself",
    "him","himself","his","how","how's","i","i'd","i'll","i'm","i've","if","in",
    "into","is","isn't","it","it's","its","itself","let's","me","more","most",
    "mustn't","my","myself","no","nor","not","of","off","on","once","only","or",
    "other","ought","our","ours","ourselves","out","over","own","same","shan't",
    "she","she'd","she'll","she's","should","shouldn't","so","some","such",
    "than","that","that's","the","their","theirs","them","themselves","then",
    "there","there's","these","they","they'd","they'll","they're","they've",
    "this","those","through","to","too","under","until","up","very","was",
    "wasn't","we","we'd","we'll","we're","we've","were","weren't","what",
    "what's","when","when's","where","where's","which","while","who","who's",
    "whom","why","why's","with","won't","would","wouldn't","you","you'd",
    "you'll","you're","you've","your","yours","yourself","yourselves" }
#use Professor's stop words file, create text file###

def stopwords(words): 
    """remove stopwords """
    result = [] 
    for w in words: 
        if w not in stopword: 
            result.append(w)
    return result

def dictonary(words2): 
    """Count how many times each word appears, If the word is new, set count to 1 If we saw it before, add 1"""
    dictionary = {}
    for word in words2: 
        if word not in dictionary: 
            dictionary[word] = 1
        else: 
            dictionary[word] += 1
    return dictionary

def most_common(dictionary):
    """Convert dictionary to list of tuples.不能改了，方便排序"""
    t = []
    for word, freq in dictionary.items():
        t.append((freq, word))
    t.sort(reverse=True)
    """sort is ranking the frequency"""
    return t


# ThinkPython-style Markov (no extra helpers)


def shift(prefix, word):
    """Return a new prefix by removing the first word and adding word at the end."""
    return prefix[1:] + (word,)

def make_markov_table(tokens, prefix_len=2):
    """
    assist by chatgpt 
    - tokens = the whole text as a list of words 
    - prefix_len = how many words we use as the state 
    - The table maps each prefix to list of possible next words 
    """
    table = {}
    if len(tokens) <= prefix_len:
        return table
    prefix = tuple(tokens[:prefix_len])
    for next_word in tokens[prefix_len:]: 
    # for each following word, record it as a possible next word for the prefix,
    # slide the prefix window forward by one word
        table.setdefault(prefix, []).append(next_word)
        prefix = shift(prefix, next_word)
    return table

def generate_markov_text(table, words_to_generate=80, seed_prefix=None):
    """
    Generate random text from a Markov table. Uses random.choice on the list of suffixes
    """
    import random
    if not table:
        return "" #if the Markov table is empty with no data, stop and return an empty string
    if seed_prefix is None or seed_prefix not in table: #if no starting prefix, then random pick one predix which is tuple of words 
        seed_prefix = random.choice(list(table.keys()))
    prefix = seed_prefix
    out = list(prefix)
    for i in range(words_to_generate):
        suffixes = table.get(prefix) # Look up all possible next words that can follow the current prefix
        if not suffixes: # if this prefix has no known next words 
        # then randomly restart with a new prefix so generation can continue
            prefix = random.choice(list(table.keys()))
            out.extend(list(prefix)) # add words the text keeps flowing
            continue
        nxt = random.choice(suffixes)
        out.append(nxt)
        prefix = shift(prefix, nxt)
    return " ".join(out)

#with chat gpt's help

def main(): 
    filename = "jane_eyre.txt"
    lines = read_data(filename)
    cleaned = clean_text(lines)
    words = cleaned.split()
    filtered_words = stopwords(words)
    word_dict = dictonary(filtered_words)
    common = most_common(word_dict)

    cleaned = clean_text(lines)
    print("\n=== Clean Text Check ===")
    print("Length of cleaned text:", len(cleaned))
    print("First 3000 characters:\n", cleaned[:3000])
### check cleaned text if it is good , with chatgpt's help 

    print("Top 10 most common words:")
    for freq, word in common[:10]:
        print(word, freq)

    # Computatig Summary Statistics 
    print("Summary statistics:")
    total_words = len(filtered_words)
    unique_words = len(word_dict)
    average_freq = sum(word_dict.values()) / len(word_dict)
    richness = unique_words / total_words 
    

    print("Total words:", total_words)
    print("Unique words:", unique_words)
    print("Average frequency:", average_freq)
    print("Vocabulary richness:", richness)
    print("Most common word:", common[0])

    total_chars = 0 
    """generate evertge word length"""
    for w in filtered_words: 
        total_chars += len(w)
    avg_word_length = total_chars / total_words
    print("Average word length:", avg_word_length)


    common = most_common(word_dict)
    print("Top 20 Word Frequency Bar Chart")
    max_freq = common[0][0]
    for freq, word in common[:20]:
        bar = "#" * int(freq / max_freq * 40)
        print(f"{word:15} | {bar} ({freq})")
### with chatgpt's help

   #with chatgpt's help
    print("\n=== Markov ===")
    tokens_for_markov = cleaned.split()   # keep stopwords for natural flow
    table = make_markov_table(tokens_for_markov, prefix_len=3)

    # Let the generator pick a random starting prefix 
    generated = generate_markov_text(table, words_to_generate=80)
    print("Generated text:\n", generated)

if __name__ == "__main__": 
    main()
